<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-2: Additional Features to PathTracer</h1>
<h2 align="middle">Shawn Zhao, Jason Gong</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="https://cal-cs184-student.github.io/project-webpages-sp23-tilenull/proj3-2/index.html">https://cal-cs184-student.github.io/project-webpages-sp23-tilenull/proj3-2/index.html</a></h2>

<br><br>

<h2 align="middle">Overview</h2>
<p>
	In this project, we implemented microfacet materials and depth of field for our pathtracer. We learned a lot about what factors go into evaluating the microfacet BSDF, and how and why the render changes with roughness, importance sampling, and different material. We also learned about how to implement a thin lens to emulate a depth of field effect, how this differs from the previous pinhole model, and how the different depth and aperature sizes vary the scene.
</p>
<br>

<h2 align="middle">Part 2: Microfacet Material</h2>

<p>
  First we implemented <code>MicrofacetBSDF::f()</code>, which evaluates the BRDF in terms of F, the Fresnal term; G, the NDF; n, the normal (0, 0, 1); and h, the half vector which we calculate by adding wi and wo and normalizing. We make sure to check that wi and wo are valid by checking if their dot products with the normal are greater than 0 and if not, we return 0. Next, we implement <code>MicrofacetBSDF::D</code>, where we define the NDF based on the Beckmann distribution in terms of alpha, which is derived from the .dae file; θh, the angle between h and the normal, determined using <code>getTheta(h)</code>; and the normal n. After that we implement <code>MicrofacetBSDF::F()</code>, the Fresnal term in terms of eta and k, which are derived from the .dae file. Finally, we implement importance sampling, covered in more depth later on. 
  <br><br>
  Shown below are images of CBdragon_microfacet_au.dae rendered at alphas of 0.005, 0.05, 0.25 and 0.5.
</p>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/dragon0005.png" align="middle" width="480px"/>
        <figcaption>0.005</figcaption>
      </td>
      <td>
        <img src="images/dragon005.png" align="middle" width="480px"/>
        <figcaption>0.05</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/dragon025.png" align="middle" width="480px"/>
        <figcaption>0.25</figcaption>
      </td>
      <td>
        <img src="images/dragon05.png" align="middle" width="480px"/>
        <figcaption>0.5</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<p>
	As the roughness increases, the dragon becomes less specular and reflective. The texture and color also appears less smooth and more matte. We also see more artifacting/noise with the lower alphas. This is because as the roughness is increased, the distribution of the microfacet normals becomes more spread out and diffuse. The increased diffusion likely also disperses the extra artifacting at lower alphas; we can see it become finer grained and blended in.
</p>

<p>
	Shown below are images of CBbunny_microfacet_cu.dae rendered with cosine hemisphere sampling vs. our implementation of importance sampling.
  </p>
  <!-- Example of including multiple figures -->
  <div align="middle">
	<table style="width:100%">
	  <tr align="center">
		<td>
		  <img src="images/bunnyhem.png" align="middle" width="480px"/>
		  <figcaption>Cosine hemisphere sampling</figcaption>
		</td>
		<td>
		  <img src="images/bunnyimp.png" align="middle" width="480px"/>
		  <figcaption>Importance sampling</figcaption>
		</td>
	  </tr>
	</table>
  </div>
  <br>

  <p>
	Cosine hemisphere sampling is considerably more noisy and artifacted than importance sampling. The reflective parts of the bunny are noisy and the black parts and the edges of the object are especially defined with cosine hemisphere sampling. Our normal distribution function in <code>MicrofacetBSDF::D()</code> is defined based on the Beckmann distribution so in our importance sampling we sample according to the shape of the Beckmann NDF, reducing noise.
  </p>
  <p>
	To implement importance sampling we calculate the pdfs pθ and pΦ and sample them using the inverted and integrated equations given in the spec, where we use <code>sampler.get_sample()</code> to determine r1 and r2.
	<div align="middle">
		<table style="width:100%">
		  <tr align="center">
			<td>
			  <img src="images/2.4.png" align="middle" width="300px"/>
			</td>
			<td>
			  <img src="images/2.4-1.png" align="middle" width="300px"/>
			</td>
		  </tr>
		</table>
	  </div>
	  <br>
	  We determine the sampled microfacet normal h by converting from spherical coordinates using the formula, where r=1:
	  <div align="middle">
		<table style="width:100%">
		  <tr align="center">
			<td>
				<img src="images/2.4-2.png" align="middle" width="200px"/>
	  <figcaption>https://en.wikipedia.org/wiki/Vector_fields_in_cylindrical_and_spherical_coordinates</figcaption>
			</td>
		  </tr>
		</table>
	  </div>
	  <br>
	  We reflect wo about h and assign it to the pointer to wi using the formula:
	  <div align="middle">
		<table style="width:100%">
		  <tr align="center">
			<td>
				<img src="images/2.4-3.png" align="middle" width="300px"/>
				<figcaption>https://en.wikipedia.org/wiki/Reflection_(mathematics)</figcaption>
			</td>
		  </tr>
		</table>
	  </div>
	  <br>
	  Finally, we calculate the pdf of sampling wi with respect to the solid angle using the formula:
	  <div align="middle">
		<table style="width:100%">
		  <tr align="center">
			<td>
				<img src="images/2.4-4.png" align="middle" width="300px"/>
				<figcaption></figcaption>
			</td>
		  </tr>
		</table>
	  </div>
	  <br>
	  We check if the sampled wi is valid by checking if its dot product with the normal is greater than 0, and if not we return a pdf and BRDF of 0.
  </p>
  <p>
	Shown below are images of CBdragon_microfacet_au.dae rendered with chromium instead of gold at alpha=0.05 and 0.5.
  </p>
  <div align="middle">
	<table style="width:100%">
	  <tr align="center">
		<td>
		  <img src="images/dragonchromium005.png" align="middle" width="480px"/>
		  <figcaption>0.05</figcaption>
		</td>
		<td>
		  <img src="images/dragonchromium05.png" align="middle" width="480px"/>
		  <figcaption>0.5</figcaption>
		</td>
	  </tr>
	</table>
  </div>
  <br>
  <p>
	We used values of eta = 3.1743, 3.1800, 2.4650 and k = 3.3000, 3.3300, 3.2150.
  </p>


<h2 align="middle">Part 4: Depth of Field</h2>
<p>
  In previous parts, we treated the camera as a pinhole camera, such that for given point we are imaging, each pixel on the image plane only receives radiance from one ray that connects the pixel and the point, passing through the pinhole. Because of this, everything is in focus. However, with a thin lens camera, each point we image has rays traced to it from every point on the lens, which then can be traced back to the image plane. For points along the plane of focus, these rays converge back to the exact same pixel on our image plane, so they are clear and in focus. However, for points not along the plane of focus, these rays don't exactly converge, which leads to blurring in the image plane. As such, simulating a thin lens model camera allows to have parameters such as aperture size and focal depth, and we can investigate how varying these affects the image.
</p>
<p>
  We can observe how changing the focus depth affects an image. Shown below is a "focus stack" on CBdragon_microfacet_au.dae, where we vary the focus depth. All images were rendered with 512 samples per pixel, 4 samples per light, a max ray depth of 12, and an aperture size of 0.23:
</p>
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/dragonb023d4.png" align="middle" width="480px"/>
        <figcaption>d = 4</figcaption>
      </td>
      <td>
        <img src="images/dragonb023d45.png" align="middle" width="480px"/>
        <figcaption>d = 4.5</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/dragonb023d5.png" align="middle" width="480px"/>
        <figcaption>d = 5</figcaption>
      </td>
      <td>
        <img src="images/dragonb023d55.png" align="middle" width="480px"/>
        <figcaption>d = 5.5</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  We notice that as we vary focus depth, the object comes in and out of focus. For d = 4, the object was out of focus because it was behind the plane of focus, but even now we can tell that the front is a little more clear than the back, which is what we expect since the front is closer to the plane of focus. For d = 4.5, we've shifted the plane of focus back a little such that the front of the dragon is in focus, but the back is not. For d = 5, we've shifted the plane of focus farther back such that the front of the dragon is now going out of focus, but the back is now in focus. Finally, for d = 5.5, we've shifted the plane of focus even farther back such that the entire object is in front of the plane of focus, but we can tell that the tail of the dragon is clearer than the head, which makes sense since it is closer to the plane of focus, and thus rays from that point would still almost converge.
</p>

<p>
  We can also observe how changing the aperture size affects an image. Shown below is CBdragon_microfacet_au.dae imaged at varying aperture sizes, all rendered with 512 samples per pixel, 4 samples per light, a max ray depth of 12, and a focus depth of 4.5:
</p>
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/dragonb01d45.png" align="middle" width="480px"/>
        <figcaption>b = 0.1</figcaption>
      </td>
      <td>
        <img src="images/dragonb03d45.png" align="middle" width="480px"/>
        <figcaption>b = 0.3</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/dragonb05d45.png" align="middle" width="480px"/>
        <figcaption>b = 0.5</figcaption>
      </td>
      <td>
        <img src="images/dragonb07d45.png" align="middle" width="480px"/>
        <figcaption>b = 0.7</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
  We notice that as the aperture size approaches 0, the image becomes more focused all around. This makes sense, as an aperture size of 0 is equivalent to a pinhole camera, and there is little blurring due to non-convergence of light rays from an imaging point. On the other hand, as the aperture size increases, the images becomes less and less focused. This makes sense as well, since as we increase the aperture size, we let in more and more radiance from points other than the specific point we want, and they start to all mix when imaging onto a specific pixel.
</p>

<h3>
	Please write a short paragraph together for your final report that describes how you collaborated, how it went, and what you learned.
  </h3>
  <p>
	We split and wrote up the initial code semi-independently but we code reviewed together and debugged collaboratively as well. We learned a lot about efficiently working as a team on a large project.
  </p>

</body>
</html>