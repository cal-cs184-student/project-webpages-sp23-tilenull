<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">Shawn Zhao, Jason Gong</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="https://cal-cs184-student.github.io/project-webpages-sp23-tilenull/proj3-1/index.html">https://cal-cs184-student.github.io/project-webpages-sp23-tilenull/proj3-1/index.html</a></h2>

<br><br>

<h2 align="middle">Overview</h2>
<p>
    In this project, we learned how to ray trace lights and render shading with light sources. We applied concepts from class such as ray-scene intersection, BVH trees, and direct/indirect lighting. We finally were able to render realistic images with proper color, and it was interesting to see our progress even in this project itself, as we went from rendering scenes to incorporating light sources into the shading.
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<p>
  In this part, we implemented ray generation and primitive intersection parts of the rendering pipeline. To generate a ray we are given (x, y) in image space to pass through from the camera origin. Image space is bounded from (0, 0) to (1, 1) with (0.5, 0.5) at the center so to convert it to camera space we must first offset x and y by -0.5 so that the plane is centered at (0, 0). The plane is now bounded by (-0.5, -0,5) and (0.5, 0.5) so we multiply by 2 in order to have it bounded from (-1, -1) to (1, 1). We then multiply by tan(0.5 * radians(hFov)) to finish the transition to camera space. To convert from camera space we multiply by camera to world rotation matrix, c2w. Finally we generate our new ray with this normalized vector as the direction, the origin at pos, and min_t and max_t as nclip and fclip.
</p>
<p>In raytrace_pixel, we take ns_aa samples of the pixel and generate rays and trace them to get a Monte Carlo estimate of the pixel. We loop over the number of samples, choosing the position of the sample in the pixel from gridSampler->get_sample() and generating a ray for this position. We average the est_radiance_global_illumination of these sampled rays and update the pixel on the sample buffer.</p>
<br>
</p>
<img src="images/mollertrumbore.png" align="middle" width="400px"/>
<p>
  For triangle intersection, we can represent the triangle in barycentric coordinates as a(P0) + b(P1) + g(P2) = (1-b-g)(P0) + b(P1) + g(P2). To find the intersection we can set this equal to the equation for a ray, O + tD, and rearranging results in the matrix system of equations shown in the image. Plugging in the values, we can determine t and the barycentric coordinates for the intersection. If the barycentric coordinates are all valid, between 0 and 1, and the t is between the min_t and max_t of the ray, then we determine that there is an intersection.
  <br><br>
  Shown below are images with normal shading for a few small .dae files:
</p>
<br>

<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/CBspheres.png" align="middle" width="480px"/>
        <figcaption>CBspheres.dae</figcaption>
      </td>
      <td>
        <img src="images/teapot.png" align="middle" width="480px"/>
        <figcaption>teapot.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/cow.png" align="middle" width="480px"/>
        <figcaption>cow.dae</figcaption>
      </td>
      <td>
        <img src="images/beetle.png" align="middle" width="480px"/>
        <figcaption>beetle.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


<h2 align="middle">Part 2: Bounding Volume Hierarchy</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<p>
  In this part, we implemented BVH construction. To construct our BVH we first loop over the primitives and enlarge the bounding box by the bounding box of each primitive, while also collecting some useful numbers to determine our heuristic with, the minimum and maximum centroids across the x, y, and z axes and the average of the centroids. Our heuristic is to pick the axis with the largest range and use the centroid of this axis as the splitting point for our left and right nodes. The idea is that the axis with the largest range will likely contain the most primitives spread out across it and taking the average splits them evenly. We calculate the range from the minimums and maximums and depending on the axis we choose we split the primitives using std::partition based on the .x, .y, or .z of the overall average centroid. We then recursively build the BVH by assigning the left and right nodes of the node to recursive calls on the partitioned halves of the primitives vector. The base case is if there are less than max_leaf_size primitives, then the node is a leaf and the left and right nodes are null.
  <br><br>
  Shown below are images with normal shading for a few large .dae files were rendered with BVH acceleration.
</p>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/maxplanck.png" align="middle" width="480px"/>
        <figcaption>maxplanck.dae</figcaption>
      </td>
      <td>
        <img src="images/CBLucy.png" align="middle" width="480px"/>
        <figcaption>CBlucy.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/bunny.png" align="middle" width="480px"/>
        <figcaption>bunny.dae</figcaption>
      </td>
      <td>
        <img src="images/beast.png" align="middle" width="480px"/>
        <figcaption>beast.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<p>
  Rendering cow.dae on 8 threads at 800x600 without BVH acceleration took 54.1692s with average 729.208034 intersection tests per ray. With BVH acceleration it took only 0.0749s with 2.131698 intersection tests per ray. Beetle.dae took 82.2256s with 1025.578253 intersection tests per ray vs 0.0721s with 1.311082 intersection tests per ray. Bench.dae took 953.2370s with 10727.354107 intersection tests per ray vs 0.0631s with 0.448200 intersection tests per ray. BVH acceleration greatly speeds up the process of rendering by allowing us to ignore rays that do not intersect the bounding boxes instead of checking every primitive individually. Our heuristic allows us to efficiently group the primitives in bounding boxes such that we can try to ignore as many primitives as possible per ray. The performance gains are more and more apparent with more complex geometries, which took more intersection tests more ray without BVH acceleration, allowing us to render some files that were not possible before. 

</p>
<br>

<h2 align="middle">Part 3: Direct Illumination</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<p>
  In this part, we implemented direct lighting. For direct lighting with hemisphere sampling we used the Monte Carlo estimator from Lecture 13:
  <img src="images/3.3eq.png" align="middle" width="400px"/>
  <br>
  We loop over num_samples samples and for each sample we sample a ray direction, <code>wj</code>, from hemisphereSampler. We use this to generate a ray from <code>hit_p</code> in the direction <code>wj</code>. Because <code>wj</code> is sampled in object space, we must convert it to world space by multiplying by the object to world matrix, <code>o2w</code> and normalize. We set the <code>min_t</code> of this new ray to <code>EPS_F</code>, a small constant that prevents numerical precision issues where the ray intersects the surface it starts from. In order to check if the new ray adds direct illumination to the light source we use <code>bvh->intersect</code>. If it does intersect, we can add this sample to our running total. We multiply <code>fr</code>, the BSDF by <code>li</code>, the incoming light calculated from <code>get_emission()</code> of the intersection and then multiply by <code>cos_theta(wj)</code> and then normalize by dividing by the probability of this sample, <code>p(wj)</code>, which is the pdf of the hemisphere, <code>1/2PI</code>. Finally, we normalize the total by dividing by the number of samples.
  <br><br>
  For direct lighting with importance sampling, we use a similar Monte Carlo estimator, except instead of sampling from a hemisphere, we loop through each light source in the scene and sample from each light. For area lights, we use <code>ns_area_light</code> number of samples, but for point lights, we only use one sample, since all samples from that point light would be the same. Similar to the hemisphere approach, we cast a ray from <code>hit_p</code> in the direction <code>w_j</code>, but this time, our <code>wj</code> is already in world space and normalized. Similarly, we set the <code>min_t</code> of the new ray to <code>EPS_F</code> to avoid precision issues, but we also set the <code>max_t</code> of the new ray to <code>distToLight - EPS_F</code> to avoid counting the light itself as an intersection. This is important because unlike the ray in the previous part, which was effectively a light ray, the new ray we cast in this algorithm is a shadow ray, and we only want to add the radiance of the direct light to a pixel if there is <b>not</b> an object in the way. If there is no object, we can use the same reflectance formula as the hemisphere algorthim and assign that radiance to the pixel.
  <br><br>
  Shown below is CB_bunny.dae rendered with hemisphere sampling and light sampling with different sampling parameters:
</p>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/bunny_hemisphere_noisy.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae rendered using hemisphere sampling with 16 camera rays per pixel and 8 samples per area light</figcaption>
      </td>
      <td>
        <img src="images/bunny_direct_noisy.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae rendered using importance sampling with 16 camera rays per pixel and 8 samples per area light</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/bunny_hemisphere_smooth.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae rendered using hemisphere sampling with 64 camera rays per pixel and 32 samples per area light</figcaption>
      </td>
      <td>
        <img src="images/bunny_direct_smooth.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae rendered using importance sampling with 64 camera rays per pixel and 32 samples per area light</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>
<p>
    We can observe that hemisphere sampling leads to much more noisy images compared to lighting sampling for the same parameters, and has much clearer shadows. Even at 64 camera rays per pixel and 32 samples per area light, hemisphere sampling is still noticeably noisy. On the other hand, light sampling is already smooth at 16 camera rays per pixel and 8 samples per area light, and even the shadows become un-pixelated at the higher resolution.
<br><br>
  Shown below is CBbunny.dae rendered with 1, 4, 16, and 64 light rays and with 1 sample per pixel using light sampling:
</p>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/bunny_1.png" align="middle" width="400px"/>
        <figcaption>1 Light Ray</figcaption>
      </td>
      <td>
        <img src="images/bunny_4.png" align="middle" width="400px"/>
        <figcaption>4 Light Rays</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/bunny_16.png" align="middle" width="400px"/>
        <figcaption>16 Light Rays</figcaption>
      </td>
      <td>
        <img src="images/bunny_64.png" align="middle" width="400px"/>
        <figcaption>64 Light Rays</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
    We observe that even at 1 light ray, light sampling leads to pretty good results compared even higher parameter hemispher sampling, and the general colors and shadows are there. The artifacting becomes less noticeable as the number of light rays increases.
</p>
<br>



<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<h3>
  Walk through your implementation of the indirect lighting function.
</h3>
<p>
    In this part, we implemented global illumination by including indirect lighting with our previously implemented direct lighting. We implemented <code>at_least_one_bounce_radiance</code> as a recursive function that takes a ray and an intersection, and returns the total amount of radiance at that intersection. That radiance includes not only the previous implemented one-bounce radiance, but also the radiance from other bounces, which can be calculated by a recursive call to <code>at_least_one_bounce_radiance</code> with a shadow ray cast from the original intersection in the direction of a vector that was sampled from the intersection's bsdf. We apply the radiance formula using the calculated radiance from the bounce and the bounce's reflectance, and add that to a running sum of the current point's radiance. At the end of all the recursive calls, we will receive the total radiance of all the bounces to a point. In order to avoid infinite recursion, we set max ray depth, but also implemented Russian Roulette. In Russian Roulette, we automatically terminate bouncing the ray with some probability.
    <br><br>
    Shown below are some images rendered with global (direct and indirect) illumination using 1024 samples per pixel, 4 light rays, and a max ray depth of 5:
</p>

<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/bunny_global.png" align="middle" width="400px"/>
        <figcaption>CBBunny.dae rendered with global illumination</figcaption>
      </td>
      <td>
        <img src="images/spheres_global.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae rendered with global illumination</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<p>
  We can observe the effects of direct and indirect illumination separately. Shown below are two images rendered with 1024 samples per pixel, 4 light rays, and a max ray depth of 5 (for the indirect illumination), one only using direct illumination, and one only using indirect illumination:
</p>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/bunny_m1.png" align="middle" width="400px"/>
        <figcaption>CBBunny.dae rendered with only direct illumination</figcaption>
      </td>
      <td>
        <img src="images/bunny_indirect.png" align="middle" width="400px"/>
        <figcaption>CBBunny.dae rendered with only indirect illumination</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    
  <br>
  <br>
  We can also observe how changing max ray depth changes a scene. Shown below are renders of CB_bunny.dae with max ray depth set to varying levels, using 1024 samples per pixel and 4 light rays:
</p>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/bunny_m0.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0</figcaption>
      </td>
      <td>
        <img src="images/bunny_m1.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/bunny_m2.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2</figcaption>
      </td>
      <td>
        <img src="images/bunny_m3.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/bunny_m100.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    We notice that a max ray depth of 0 is equivalent to zero-bounce radiance, and a max ray depth of 1 is equivalent to direct illumination. We also observe that as we increase max ray depth, the scene gets brighter and brighter, but diminishingly so, as each bounce away contributes less and less radiance to the total radiance of a pixel due to dissipation.
<br><br>
  We can also observe how changing the number of samples per pixel changes a scene. Shown below are renders of CBspheres_lambertian.dae with samples per pixel set to varying levels, using 4 light rays and a max ray depth of 5.
</p>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/spheres_s1.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel</figcaption>
      </td>
      <td>
        <img src="images/spheres_s2.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/spheres_s4.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel</figcaption>
      </td>
      <td>
        <img src="images/spheres_s8.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/spheres_s16.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel</figcaption>
      </td>
      <td>
        <img src="images/spheres_s64.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/spheres_s1024.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
    We notice that as the samples per pixel increases, the noise in the render decreases, until the images has no noticeable artifacts at 1024 samples per pixel.
</p>
<br>


<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<h3>
  Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
</h3>
<p>
    In this part, we implemented adaptive sampling. Adaptive sampling takes into account how fast a pixel converges and concentrates the samples in parts of the image that converge slower, so that we don't have to sample an unnecessarily high amount of times for pixels that converge in few samples. We implemented this through a 95% confidence interval that our pixel has converged, by taking <code>I = 1.96 * (standard deviation)/sqrt(number of samples)</code>. Intuitively, <code>I</code> is small if the number of samples is large or if the variance is small, and either condition being satisfied leads us to believe that the pixel has converged. Therefore, we check if <code>I <= max tolerance * mean</code>, and terminate the raytracing for this pixel if this condition is satisfied. 
    <br><br>
    Shown below are renders with a corresponding heatmap of their sampling rates, with red being a lot of samples and blue being a few amount of samples. The renders used 2048 samples per pixel, 1 light ray, and a max ray depth of 5:
</p>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/bunny_adaptive.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/bunny_adaptive_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/spheres_adaptive.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/spheres_adaptive_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  We observed that uninteresting parts of the scene, such as the walls, the light itself, and the direct lighting pixels have low sampling rates, but shadows and undersides of objects have a high sampling rate.
</p>

<h3>
  Please write a short paragraph together for your final report that describes how you collaborated, how it went, and what you learned..
</h3>
<p>
  We split and wrote up the initial code semi-independently but we code reviewed together and debugged collaboratively as well. Through this project we learned a lot about ray tracing lights and rendering shading with light sources, from basic primitive intersection to BVH acceleration, lighting, and adaptive sampling. We also learned a lot about efficiently working as a team on a large project.
</p>

</body>
</html>